{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30583c6f",
   "metadata": {},
   "source": [
    "批量归⼀化（batch normalization）层，它能让较深的神经⽹络的训练变得更加容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faccd3d",
   "metadata": {},
   "source": [
    "通常来说，数据标准化预处理对于浅层模型就⾜够有效了。随着模型训练的进⾏，当每层中参数更新\n",
    "时，靠近输出层的输出较难出现剧烈变化。但对深层神经⽹络来说，即使输⼊数据已做标准化，训练中\n",
    "模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以\n",
    "训练出有效的深度模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77551919",
   "metadata": {},
   "source": [
    "# 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db50acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batch_norm(is_training,X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    if not is_training:\n",
    "        #如果是预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat=(X-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)\n",
    "        if len(X.shape)==2:\n",
    "            #使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean=X.mean(dim=0)\n",
    "            var=((X-mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            #使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持X的形状以便后面可以做广播计算\n",
    "            mean=X.mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "            var=((X-mean)**2).mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "        #训练模式下用当前的均值和方差做标准化\n",
    "        X_hat=(X-mean)/torch.sqrt(var+eps)\n",
    "        #更新移动平均的均值和方差\n",
    "        moving_mean=momentum*moving_mean+(1.0-momentum)*mean\n",
    "        moving_var=momentum*moving_var+(1.0-momentum)*var\n",
    "    Y=gamma*X_hat+beta#拉伸和偏移\n",
    "    return Y,moving_mean,moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb7260",
   "metadata": {},
   "source": [
    "num_features 参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的 num_dims 参数对于全连接层和卷积层来说分别为2和4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3703ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_features,num_dims):\n",
    "        super(BatchNorm,self).__init__()\n",
    "        if num_dims==2:\n",
    "            shape=(1,num_features)\n",
    "        else:\n",
    "            shape=(1,num_features,1,1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma=nn.Parameter(torch.ones(shape))\n",
    "        self.beta=nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean=torch.zeros(shape)\n",
    "        self.moving_var=torch.zeros(shape)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device!=X.device:\n",
    "            self.moving_mean=self.moving_mean.to(X.device)\n",
    "            self.moving_var=self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调⽤.eval()后设成false\n",
    "        Y,self.moving_mean,self.moving_var=batch_norm(self.training,X,self.gamma,self.beta,self.moving_mean,self.moving_var,eps=1e-5,momentum=0.9)\n",
    "        return Y  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e3d129",
   "metadata": {},
   "source": [
    "# 使用批量归一化层的LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a86091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf96f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(\n",
    "    nn.Conv2d(1,6,5),\n",
    "    BatchNorm(6,num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Conv2d(6,16,5),\n",
    "    BatchNorm(16,num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    FlattenLayer(),\n",
    "    nn.Linear(16*4*4,120),\n",
    "    BatchNorm(120,num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120,84),\n",
    "    BatchNorm(84,num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84,10)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57da6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "def load_data_fashion_mnist(batch_size,resize=None,root='./Datasets/FashionMNIST'):\n",
    "    trans=[]\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform=torchvision.transforms.Compose(trans)\n",
    "    mnist_train=torchvision.datasets.FashionMNIST(root=root,train=True,download=True,transform=transform)\n",
    "    mnist_test=torchvision.datasets.FashionMNIST(root=root,train=False,download=True,transform=transform)\n",
    "    train_iter=torch.utils.data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "    test_iter=torch.utils.data.DataLoader(mnist_test,batch_size=batch_size,shuffle=False,num_workers=0)\n",
    "    return train_iter,test_iter\n",
    "\n",
    "batch_size=256\n",
    "train_iter,test_iter=load_data_fashion_mnist(batch_size=batch_size)\n",
    "lr,num_epochs=0.001,5\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87217205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net,device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    acc_sum,n=0.0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            if isinstance(net,torch.nn.Module):\n",
    "                net.eval()#评估模式，这会关闭dropout\n",
    "                acc_sum+=(net(X.to(device)).argmax(dim=1)==y.to(device)).float().sum().cpu().item()\n",
    "                net.train()#改回训练模式\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varname):\n",
    "                    acc_sum+=(net(X,is_training=False).argmax(dim=1)==y).float().sum().item()\n",
    "            n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "\n",
    "def train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs):\n",
    "    net=net.to(device)\n",
    "    print('training on ',device)\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    batch_count=0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.cpu().item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "            batch_count+=1\n",
    "        test_acc=evaluate_accuracy(test_iter,net)\n",
    "        print('epoch %d,loss %.4f,train acc %.3f,test acc %.3f,time %.lf sec'%(epoch+1,train_l_sum/batch_count,train_acc_sum/n,test_acc,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34e3860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1,loss 1.0246,train acc 0.773,test acc 0.835,time 18 sec\n",
      "epoch 2,loss 0.2328,train acc 0.864,test acc 0.818,time 15 sec\n",
      "epoch 3,loss 0.1247,train acc 0.876,test acc 0.837,time 15 sec\n",
      "epoch 4,loss 0.0836,train acc 0.885,test acc 0.851,time 15 sec\n",
      "epoch 5,loss 0.0631,train acc 0.890,test acc 0.870,time 15 sec\n"
     ]
    }
   ],
   "source": [
    "train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeba1c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0511, 1.0633, 1.2582, 1.0428, 1.3367, 0.8942], device='cuda:0',\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor([ 0.1553, -0.6250,  0.2005,  0.4576,  0.2845, -0.4047], device='cuda:0',\n",
       "        grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma.view((-1,)),net[1].beta.view((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db6ca9",
   "metadata": {},
   "source": [
    "# 简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5384e9",
   "metadata": {},
   "source": [
    "与我们刚刚⾃⼰定义的 BatchNorm 类 相 ⽐ ， Pytorch 中 nn 模 块 定 义 的 BatchNorm1d 和BatchNorm2d 类 使 ⽤ 起 来 更 加 简 单 ， ⼆ 者 分 别 ⽤ 于 全 连 接 层 和 卷 积 层 ， 都 需 要 指 定 输 ⼊ 的num_features 参数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c7db632",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(\n",
    "    nn.Conv2d(1,6,5),\n",
    "    nn.BatchNorm2d(6),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Conv2d(6,16,5),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    FlattenLayer(),\n",
    "    nn.Linear(16*4*4,120),\n",
    "    nn.BatchNorm1d(120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120,84),\n",
    "    nn.BatchNorm1d(84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1881ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "def load_data_fashion_mnist(batch_size,resize=None,root='./Datasets/FashionMNIST'):\n",
    "    trans=[]\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform=torchvision.transforms.Compose(trans)\n",
    "    mnist_train=torchvision.datasets.FashionMNIST(root=root,train=True,download=True,transform=transform)\n",
    "    mnist_test=torchvision.datasets.FashionMNIST(root=root,train=False,download=True,transform=transform)\n",
    "    train_iter=torch.utils.data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "    test_iter=torch.utils.data.DataLoader(mnist_test,batch_size=batch_size,shuffle=False,num_workers=0)\n",
    "    return train_iter,test_iter\n",
    "\n",
    "batch_size=256\n",
    "train_iter,test_iter=load_data_fashion_mnist(batch_size=batch_size)\n",
    "lr,num_epochs=0.001,5\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cb1bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net,device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    acc_sum,n=0.0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            if isinstance(net,torch.nn.Module):\n",
    "                net.eval()#评估模式，这会关闭dropout\n",
    "                acc_sum+=(net(X.to(device)).argmax(dim=1)==y.to(device)).float().sum().cpu().item()\n",
    "                net.train()#改回训练模式\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varname):\n",
    "                    acc_sum+=(net(X,is_training=False).argmax(dim=1)==y).float().sum().item()\n",
    "            n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "\n",
    "def train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs):\n",
    "    net=net.to(device)\n",
    "    print('training on ',device)\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    batch_count=0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.cpu().item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "            batch_count+=1\n",
    "        test_acc=evaluate_accuracy(test_iter,net)\n",
    "        print('epoch %d,loss %.4f,train acc %.3f,test acc %.3f,time %.lf sec'%(epoch+1,train_l_sum/batch_count,train_acc_sum/n,test_acc,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1357bbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1,loss 0.9968,train acc 0.792,test acc 0.763,time 12 sec\n",
      "epoch 2,loss 0.2276,train acc 0.865,test acc 0.862,time 11 sec\n",
      "epoch 3,loss 0.1216,train acc 0.879,test acc 0.840,time 11 sec\n",
      "epoch 4,loss 0.0818,train acc 0.887,test acc 0.864,time 11 sec\n",
      "epoch 5,loss 0.0617,train acc 0.892,test acc 0.858,time 11 sec\n"
     ]
    }
   ],
   "source": [
    "train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
