{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef7703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1727317b",
   "metadata": {},
   "source": [
    "# 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ee5f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences:42068'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert 'ptb.train.txt' in os.listdir('./data')\n",
    "with open('./data/ptb.train.txt','r') as f:\n",
    "    lines=f.readlines()\n",
    "    raw_dataset=[st.split() for st in lines]\n",
    "'# sentences:%d'%len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "060ec9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "#tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
      "#tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "for st in raw_dataset[:3]:\n",
    "    print('#tokens:',len(st),st[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9a022",
   "metadata": {},
   "source": [
    "## 建立词语索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6120f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter=dict(filter(lambda x:x[1]>=5,counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84de853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#tokens:887100'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_token=[tk for tk,_ in counter.items()]\n",
    "token_to_idx={tk:idx for idx,tk in enumerate(idx_to_token)}\n",
    "dataset=[[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "num_tokens=sum([len(st) for st in dataset])\n",
    "'#tokens:%d'%num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43e187",
   "metadata": {},
   "source": [
    "## 二次采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ed4f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#tokens:375360'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0,1)<1-math.sqrt(1e-4/counter[idx_to_token[idx]]*num_tokens)\n",
    "subsampled_dataset=[[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "'#tokens:%d'%sum([len(st) for st in subsampled_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3737cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#the:before=50770,after=2083'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return '#%s:before=%d,after=%d'%(token,sum([st.count(token_to_idx[token]) for st in dataset]),sum([st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b32cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#join:before=45,after=45'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91328eda",
   "metadata": {},
   "source": [
    "## 提取中心词和背景词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c810c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset,max_window_size):\n",
    "    centers,contexts=[],[]\n",
    "    for st in dataset:\n",
    "        if len(st)<2:\n",
    "            continue\n",
    "        centers +=st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size=random.randint(1,max_window_size)\n",
    "            indices=list(range(max(0,center_i-window_size),min(len(st),center_i+1+window_size)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers,contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "564ff2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [1, 3]\n",
      "center 3 has contexts [2, 4]\n",
      "center 4 has contexts [2, 3, 5, 6]\n",
      "center 5 has contexts [3, 4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset=[list(range(7)),list(range(7,10))]\n",
    "print('dataset',tiny_dataset)\n",
    "for center,context in zip(*get_centers_and_contexts(tiny_dataset,2)):\n",
    "    print('center',center,'has contexts',context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31db11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers,all_contexts=get_centers_and_contexts(subsampled_dataset,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c099b",
   "metadata": {},
   "source": [
    "# 负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f5b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts,sampling_weights,K):\n",
    "    all_negatives,neg_candidates,i=[],[],0\n",
    "    population=list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives=[]\n",
    "        while len(negatives)<len(contexts)*K:\n",
    "            if i == len(neg_candidates):\n",
    "                i,neg_candidates=0,random.choices(population,sampling_weights,k=int(1e5))\n",
    "            neg,i=neg_candidates[i],i+1\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "sampling_weights=[counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives=get_negatives(all_contexts,sampling_weights,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98819b4d",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55ae356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,centers,contexts,negatives):\n",
    "        assert len(centers)==len(contexts)==len(negatives)\n",
    "        self.centers=centers\n",
    "        self.contexts=contexts\n",
    "        self.negatives=negatives\n",
    "    def __getitem__(self,index):\n",
    "        return (self.centers[index],self.contexts[index],self.negatives[index])\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617f8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    max_len=max(len(c)+len(n) for _,c,n in data)\n",
    "    centers,contexts_negatives,masks,labels=[],[],[],[]\n",
    "    for center,context,negative in data:\n",
    "        cur_len=len(context)+len(negative)\n",
    "        centers+=[center]\n",
    "        contexts_negatives+=[context+negative+[0]*(max_len-cur_len)]\n",
    "        masks+=[[1]*cur_len+[0]*(max_len-cur_len)]\n",
    "        labels+=[[1]*len(context)+[0]*(max_len-len(context))]\n",
    "    return (torch.tensor(centers).view(-1,1),torch.tensor(contexts_negatives),torch.tensor(masks),torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25656b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size=512\n",
    "num_workers=0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "dataset=MyDataset(all_centers,all_contexts,all_negatives)\n",
    "data_iter=Data.DataLoader(dataset,batch_size,shuffle=True,collate_fn=batchify,num_workers=num_workers)\n",
    "\n",
    "for batch in data_iter:\n",
    "    for name,data in zip(['centers','contexts_negatives','masks','labels'],batch):\n",
    "        print(name,'shape:',data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e817f92",
   "metadata": {},
   "source": [
    "# 跳字模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b61c4",
   "metadata": {},
   "source": [
    "## 嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fa53acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.5031, -0.1519,  1.3063,  0.2633],\n",
       "        [-2.1616, -1.2789,  0.1801,  1.6020],\n",
       "        [ 1.3239, -1.4238,  0.3973,  0.3316],\n",
       "        [-1.6827, -1.6508,  0.3905, -2.0055],\n",
       "        [ 0.7390,  0.0503,  0.1149, -0.8774],\n",
       "        [-1.0615, -0.7423, -0.2248, -0.5482],\n",
       "        [-0.1959,  1.0133,  0.7691,  0.3495],\n",
       "        [-0.9932, -0.0643, -1.4158,  1.8086],\n",
       "        [-0.9041,  0.9900,  2.1469, -0.9400],\n",
       "        [ 1.1303,  0.1270,  1.7050, -2.0273],\n",
       "        [-0.2039, -0.2043,  0.9505,  0.7859],\n",
       "        [ 0.6883, -1.2262,  1.1897,  0.0901],\n",
       "        [-1.0786, -0.8114,  0.4416, -0.1583],\n",
       "        [ 0.3023, -0.6352,  0.1206,  0.2482],\n",
       "        [-2.0119,  1.2533, -0.8425,  0.3436],\n",
       "        [ 1.2651,  0.3204,  0.7677,  0.0709],\n",
       "        [ 2.2074, -0.6789, -0.1773, -0.7628],\n",
       "        [ 1.5231,  0.2384,  1.6155, -0.2701],\n",
       "        [ 1.1645, -1.1776, -1.4131, -2.0500],\n",
       "        [ 0.0127,  1.1089,  1.8409,  0.6716]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed=nn.Embedding(num_embeddings=20,embedding_dim=4)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d96d98ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1616, -1.2789,  0.1801,  1.6020],\n",
       "         [ 1.3239, -1.4238,  0.3973,  0.3316],\n",
       "         [-1.6827, -1.6508,  0.3905, -2.0055]],\n",
       "\n",
       "        [[ 0.7390,  0.0503,  0.1149, -0.8774],\n",
       "         [-1.0615, -0.7423, -0.2248, -0.5482],\n",
       "         [-0.1959,  1.0133,  0.7691,  0.3495]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.long)\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa594e70",
   "metadata": {},
   "source": [
    "## 小批量乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "917f21a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.ones((2,1,4))\n",
    "y=torch.ones((2,4,6))\n",
    "torch.bmm(x,y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d10e38",
   "metadata": {},
   "source": [
    "## 跳字模型前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c36efa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center,contexts_and_negatives,embed_v,embed_u):\n",
    "    v=embed_v(center)\n",
    "    u=embed_u(contexts_and_negatives)\n",
    "    pred=torch.bmm(v,u.permute(0,2,1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0cef35",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b58ddc",
   "metadata": {},
   "source": [
    "## 二元交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "447a980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss,self).__init__()\n",
    "    def forward(self,inputs,targets,mask=None):\n",
    "        inputs,targets,mask=inputs.float(),targets.float(),mask.float()\n",
    "        res=nn.functional.binary_cross_entropy_with_logits(inputs,targets,reduction='none',weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "loss=SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "549338fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=torch.tensor([[1.5,0.3,-1,2],[1.1,-0.6,2.2,0.4]])\n",
    "label=torch.tensor([[1,0,0,0],[1,1,0,0]])\n",
    "mask=torch.tensor([[1,1,1,1],[1,1,1,0]])\n",
    "loss(pred,label,mask)*mask.shape[1]/mask.float().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8ec12bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\n",
      "1.2100\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return -math.log(1/(1+math.exp(-x)))\n",
    "print('%.4f'%((sigmd(1.5)+sigmd(-0.3)+sigmd(1)+sigmd(-2))/4))\n",
    "print('%.4f'%((sigmd(1.1)+sigmd(-0.6)+sigmd(-2.2))/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070ee2d",
   "metadata": {},
   "source": [
    "## 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ddc5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=100\n",
    "net=nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token),embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token),embedding_dim=embed_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72a138",
   "metadata": {},
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4e65374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,lr,num_epochs):\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('train on',device)\n",
    "    net=net.to(device)\n",
    "    optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start,l_sum,n=time.time(),0.0,0\n",
    "        for batch in data_iter:\n",
    "            center,context_negative,mask,label=[d.to(device) for d in batch]\n",
    "            pred=skip_gram(center,context_negative,net[0],net[1])\n",
    "            l=(loss(pred.view(label.shape),label,mask)*mask.shape[1]/mask.float().sum(dim=1)).mean()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum+=l.cpu().item()\n",
    "            n+=1\n",
    "        print('epoch %d,loss %.2f,time %.2fs'%(epoch+1,l_sum/n,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d46cb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 1,loss 1.97,time 26.72s\n",
      "epoch 2,loss 0.62,time 24.37s\n",
      "epoch 3,loss 0.45,time 24.38s\n",
      "epoch 4,loss 0.39,time 24.30s\n",
      "epoch 5,loss 0.37,time 24.35s\n",
      "epoch 6,loss 0.35,time 24.26s\n",
      "epoch 7,loss 0.34,time 24.23s\n",
      "epoch 8,loss 0.33,time 25.00s\n",
      "epoch 9,loss 0.32,time 24.30s\n",
      "epoch 10,loss 0.32,time 24.12s\n"
     ]
    }
   ],
   "source": [
    "train(net,0.01,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6707fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9858, 100])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(net[0].weight.shape)\n",
    "print(net[0].weight[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a22511",
   "metadata": {},
   "source": [
    "# 应用词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "713eecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131 1059 8802   55]\n",
      "cosine sim=0.410:computer\n",
      "cosine sim=0.403:newsletter\n",
      "cosine sim=0.387:than\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token,k,embed):\n",
    "    W=embed.weight.data\n",
    "    x=W[token_to_idx[query_token]]\n",
    "    cos=torch.matmul(W,x)/(torch.sum(W*W,dim=1)*torch.sum(x*x)+1e-9).sqrt()\n",
    "    _,topk=torch.topk(cos,k=k+1)\n",
    "    topk=topk.cpu().numpy()\n",
    "    print(topk)\n",
    "    for i in topk[1:]:\n",
    "        print('cosine sim=%.3f:%s'%(cos[i],(idx_to_token[i])))\n",
    "get_similar_tokens('chip',3,net[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
