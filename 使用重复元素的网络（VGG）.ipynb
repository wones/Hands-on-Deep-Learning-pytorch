{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180c3f76",
   "metadata": {},
   "source": [
    "# VGG块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964192d",
   "metadata": {},
   "source": [
    "VGG块的组成规律是：连续使⽤数个相同的填充为1、窗⼝形状为3x3 的卷积层后接上⼀个步幅为2、窗⼝形状为2x2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，⽽池化层则对其减半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ccb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c258a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    blk=[]\n",
    "    for i in range(num_convs):\n",
    "        if i==0:\n",
    "            blk.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1))\n",
    "        blk.append(nn.ReLU())\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9181b7",
   "metadata": {},
   "source": [
    "# VGG网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140b77e",
   "metadata": {},
   "source": [
    "与AlexNet和LeNet⼀样，VGG⽹络由卷积层模块后接全连接层模块构成。卷积层模块串联数个 vgg_block ，其超参数由变量 conv_arch 定义。该变量指定了每个VGG块⾥卷积层个数和输⼊输出通道数。全连接模块则跟AlexNet中的⼀样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b33fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch=((1,1,64),(1,64,128),(2,128,256),(2,256,512),(2,512,512))\n",
    "#经过5个vgg_block, 宽⾼会减半5次, 变成 224/32 = 7\n",
    "fc_features=512*7*7 \n",
    "fc_hidden_units=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c761b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b485e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch,fc_features,fc_hidden_units=4096):\n",
    "    net=nn.Sequential()\n",
    "    for i,(num_conv,in_channels,out_channels) in enumerate(conv_arch):\n",
    "        net.add_module('vgg_block_'+str(i+1),vgg_block(num_conv,in_channels,out_channels))\n",
    "    net.add_module('fc',nn.Sequential(FlattenLayer(),\n",
    "                                      nn.Linear(fc_features,fc_hidden_units),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.5),\n",
    "                                      nn.Linear(fc_hidden_units,fc_hidden_units),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.5),\n",
    "                                      nn.Linear(fc_hidden_units,10)\n",
    "                                     ))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "582eec61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape: torch.Size([2, 64, 112, 112])\n",
      "vgg_block_2 output shape: torch.Size([2, 128, 56, 56])\n",
      "vgg_block_3 output shape: torch.Size([2, 256, 28, 28])\n",
      "vgg_block_4 output shape: torch.Size([2, 512, 14, 14])\n",
      "vgg_block_5 output shape: torch.Size([2, 512, 7, 7])\n",
      "fc output shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "net=vgg(conv_arch,fc_features,fc_hidden_units)\n",
    "X=torch.rand(2,1,224,224)\n",
    "# named_children获取⼀级⼦模块及其名字(named_modules会返回所有⼦模块,包括⼦模块的⼦模块)\n",
    "for name,blk in net.named_children():\n",
    "    X=blk(X)\n",
    "    print(name,'output shape:',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19db93",
   "metadata": {},
   "source": [
    "# 获取数据和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc67125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ratio=8\n",
    "small_conv_arch=[(1,1,64//ratio),(1,64//ratio,128//ratio),(2,128//ratio,256//ratio),(2,256//ratio,512//ratio),(2,512//ratio,512//ratio)]\n",
    "net=vgg(small_conv_arch,fc_features//ratio,fc_hidden_units//ratio)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdedf629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import sys\n",
    "import torchvision.transforms as transforms\n",
    "def load_data_fashion_mnist(batch_size,resize=None,root='./Datasets/FashionMNIST'):\n",
    "    trans=[]\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform=torchvision.transforms.Compose(trans)\n",
    "    mnist_train=torchvision.datasets.FashionMNIST(root=root,train=True,download=True,transform=transform)\n",
    "    mnist_test=torchvision.datasets.FashionMNIST(root=root,train=False,download=True,transform=transform)\n",
    "    train_iter=torch.utils.data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "    test_iter=torch.utils.data.DataLoader(mnist_test,batch_size=batch_size,shuffle=False,num_workers=0)\n",
    "    return train_iter,test_iter\n",
    "\n",
    "batch_size=64\n",
    "train_iter,test_iter=load_data_fashion_mnist(batch_size,resize=224)\n",
    "lr,num_epochs=0.001,5\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0495aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net,device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    acc_sum,n=0.0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            if isinstance(net,torch.nn.Module):\n",
    "                net.eval()#评估模式，这会关闭dropout\n",
    "                acc_sum+=(net(X.to(device)).argmax(dim=1)==y.to(device)).float().sum().cpu().item()\n",
    "                net.train()#改回训练模式\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varname):\n",
    "                    acc_sum+=(net(X,is_training=False).argmax(dim=1)==y).float().sum().item()\n",
    "            n+=y.shape[0]\n",
    "    return acc_sum/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52ceaed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs):\n",
    "    net=net.to(device)\n",
    "    print('training on ',device)\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    batch_count=0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.cpu().item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "            batch_count+=1\n",
    "        test_acc=evaluate_accuracy(test_iter,net)\n",
    "        print('epoch %d,loss %.4f,train acc %.3f,test acc %.3f,time %.lf sec'%(epoch+1,train_l_sum/batch_count,train_acc_sum/n,test_acc,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bca2a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1,loss 2.3032,train acc 0.101,test acc 0.100,time 569 sec\n",
      "epoch 2,loss 1.1515,train acc 0.100,test acc 0.100,time 565 sec\n",
      "epoch 3,loss 0.7676,train acc 0.099,test acc 0.100,time 564 sec\n",
      "epoch 4,loss 0.5757,train acc 0.098,test acc 0.100,time 563 sec\n",
      "epoch 5,loss 0.4606,train acc 0.098,test acc 0.100,time 564 sec\n"
     ]
    }
   ],
   "source": [
    "train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
